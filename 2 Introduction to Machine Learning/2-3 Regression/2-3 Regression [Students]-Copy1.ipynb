{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science] \n",
    "## 2-3 Regression - Student Version\n",
    "\n",
    "In this lab, we are going to cover **Regression** methods. Supervised machine learning can be divided into classification and regression. We will begin with regression as a review of your previous statistics courses. This lab will introduce the regression methods available in the scikit-learn extension to scipy, focusing on ordinary least squares linear regression, LASSO, and Ridge regression.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "1 - [Data Splitting Review](#section_1)\n",
    "\n",
    "2 - [Linear Regression](#section_2)\n",
    "\n",
    "3 - [Ridge Regression](#section_3)\n",
    "\n",
    "4 - [LASSO Regression](#section_4)\n",
    "\n",
    "5 - [Hyperparameter Tuning](#section_5)\n",
    "\n",
    "6 - [Choosing a Model](#section_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your time at Cal, you've probably passed by one of the many bike sharing station around campus. Bike sharing systems have become more and more popular as traffic and concerns about global warming rise. This lab's data describes one such bike sharing system in Washington D.C., from [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike = pd.read_csv('../../data/day.csv')\n",
    "\n",
    "# reformat the date column to integers representing the day of the year, 001-366\n",
    "bike['dteday'] = pd.to_datetime(np.array(bike['dteday'])).strftime('%j')\n",
    "\n",
    "# get rid of the index column\n",
    "bike = bike.drop('instant', axis=1)\n",
    "\n",
    "bike.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to get familiar with the data set. In data science, you'll often hear rows referred to as **records** and columns as **features**. Before you continue, make sure you can answer the following:\n",
    "\n",
    "- How many records are in this data set?\n",
    "- What does each record represent?\n",
    "- What are the different features?\n",
    "- How is each feature represented? What values does it take, and what are the data types of each value?\n",
    "\n",
    "Explore the dataset and answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data set here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Test-Train-Validation Split  <a id='section_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from last week that before we make predictions, we need to split our data first. Prepare the bike dataset by creating a dataframe **X** with all of the features (exclude anything that is not a rider count), and a series, **y** with the *total number of riders*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "X = ...\n",
    "\n",
    "# the number of riders\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the random seed using `np.random.seed(...)`. This will affect the way numpy pseudo-randomly generates the numbers it uses to decide how to split the data into training and test sets. Any seed number is fine- the important thing is to document the number you used in case we need to recreate this pseudorandom split in the future.\n",
    "\n",
    "Then, call `train_test_split` on your X and y. Also set the parameters `train_size=` and `test_size=` to set aside 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validation Set\n",
    "\n",
    "Recall that our test data should only be used once: after our model has been selected, trained, and tweaked. Unfortunately, it's possible that in the process of tweaking our model, we could still overfit it to the training data and only find out when we return a poor test data score. What then?\n",
    "\n",
    "A **validation set** can help here. By trying your trained models on a validation set, you can (hopefully) weed out models that don't generalize well.\n",
    "\n",
    "Call `train_test_split` again, this time on your X_train and y_train. We want to set aside 25% of the data to go to our validation set, and keep the remaining 75% for our training set.\n",
    "\n",
    "Note: This means that out of the original data, 20% is for testing, 20% is for validation, and 60% is for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "# Returns 4 values: X_train, X_validate, y_train, y_validate\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression (Ordinary Least Squares) <a id='section_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to start training models and making predictions. We'll start with a **linear regression** model.\n",
    "\n",
    "[Scikit-learn's linear regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) is built around scipy's ordinary least squares, which you used in the last lab. The syntax for each scikit-learn model is very similar:\n",
    "1. Create a model by calling its constructor function. For example, `LinearRegression()` makes a linear regression model.\n",
    "2. Train the model on your training data by calling `.fit(train_X, train_y)` on the model\n",
    "\n",
    "Create a linear regression model in the cell below, and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "lin_reg = ...\n",
    "\n",
    "# fit the model\n",
    "lin_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model fit, you can look at the best-fit slope for each feature using `.coef_`, and you can get the intercept of the regression line with `.intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lin_model.coef_)\n",
    "print(lin_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the coefficients. Fill in the code below to produce a bar plot for the coefficients.\n",
    "Reminder: Seaborn's barplot takes arguments for `x`, `y` and `data`. You can take `x` and `y` from the DataFrame created below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the coefficient and feature names\n",
    "lin_reg_data = pd.DataFrame([lin_model.coef_, X.columns]).T\n",
    "lin_reg_data.columns = ['Coefficient', 'Feature']\n",
    "# Plot\n",
    "ax = ...\n",
    "ax.set_title(\"OLS Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a sense of how good our model is. We can do this by looking at the difference between the predicted values and the actual values, also called the error.\n",
    "\n",
    "We can see this graphically using a scatter plot.\n",
    "\n",
    "- Call `.predict()` on your linear regression model (`lin_model`), using your validation X, to return a list of predicted number of riders per hour. Save it to a variable `lin_pred`.\n",
    "- Using a scatter plot (`plt.scatter(...)`), plot the predicted values against the actual values (`y_validate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the number of riders\n",
    "lin_pred = ...\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "...\n",
    "plt.title('Linear Model (OLS) Predicted v. Actual')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what should our scatter plot look like if our model was 100% accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a sense of how well our model is doing by calculating the **root mean squared error**. The root mean squared error (RMSE) represents the average difference between the predicted and the actual values.\n",
    "\n",
    "To get the RMSE:\n",
    "- subtract each predicted value from its corresponding actual value (the errors)\n",
    "- square each error (this prevents negative errors from cancelling positive errors)\n",
    "- average the squared errors\n",
    "- take the square root of the average (this gets the error back in the original units)\n",
    "\n",
    "Write a function `rmse` that calculates the mean squared error of a predicted set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, actual):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the mean squared error for your linear model (`lin_pred`), using the y validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(lin_pred, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression <a id='section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've gone through the process for OLS linear regression, it's easy to do the same for [**Ridge Regression**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). In this case, the constructor function that makes the model is `Ridge()`.\n",
    "\n",
    "*Tip: the `alpha` parameter controls the regularization strength. Its default is 1 -- try changing it to see what happens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and fit a Ridge regression model\n",
    "ridge_reg = ...\n",
    "ridge_model = ...\n",
    "ridge_reg_data = pd.DataFrame([ridge_model.coef_, X.columns]).T\n",
    "ridge_reg_data.columns = ['Coefficient', 'Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients for the Ridge model. How do they compare to the coefficients for OLS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.subplots_adjust(wspace = .5, hspace=.5)\n",
    "figure.add_subplot(1, 2, 1)\n",
    "# Plot for ridge coefficients here\n",
    "...\n",
    "figure.add_subplot(1, 2, 2)\n",
    "# Plot for OLS coefficients here (to compare)\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your Ridge model to make predictions and visualize the predictions against the actual values. How does the RMSE compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "ridge_pred = ...\n",
    "\n",
    "# plot the predictions\n",
    "...\n",
    "plt.title('Ridge Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rmse for the Ridge model\n",
    "rmse(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the documentation for Ridge regression shows it has lots of **hyperparameters**: values we can choose when the model is made. Now that we've tried it using the defaults, look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). In a bit, we will try changing some parameters to see if we can get a lower RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LASSO Regression <a id='section_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll try using [LASSO regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). The constructor function to make the model is `Lasso()`. \n",
    "\n",
    "You may get a warning message saying the objective did not converge. The model will still work, but to get convergence try increasing the number of iterations (`max_iter=`) when you construct the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "lasso_reg = ...\n",
    "\n",
    "lasso_model = ...\n",
    "lasso_reg_data = pd.DataFrame([lasso_model.coef_, X.columns]).T\n",
    "lasso_reg_data.columns = ['Coefficient', 'Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients for Ridge and LASSO. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.subplots_adjust(wspace = .5, hspace=.5)\n",
    "figure.add_subplot(1, 2, 1)\n",
    "# Plot for LASSO coefficients here\n",
    "...\n",
    "figure.add_subplot(1, 2, 2)\n",
    "# Plot for Ridge coefficients here\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your LASSO model to make predictions and visualize the predictions against the actual values. How does the RMSE compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "lasso_pred = ...\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(..., ...)\n",
    "plt.title('LASSO Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rmse for the LASSO model\n",
    "rmse(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How do these three models compare on performance? What sorts of things could we do to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning <a id='section_5'></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the documentation, you might have noticed that there were a number of arguments that you could supply to each algorithm. How do we decide what the optimal settings are? This process is known as **[hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)**. Hyperparameters are essentially settings that control how the machine learning algorithm learns relationships between features and targets, and are fixed by the analyst. Here, we are going to learn how to accomplish this task using **grid search**. Grid search means we specify a list of hyperparameter specifications that we want to try out, and then we search through all combinations of these specifications. Luckily, Python creates an easy way to do this with its [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html) method. This approach combines doing an exhaustive search of hyperparameter values with cross-validation.\n",
    "\n",
    "Look at the documentation for linear regression, and fill in the hyperparameters below. Note that the `.get_params()` method will show you the hyperparameters available for your different regressor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'fit_intercept': ['True', 'False']}\n",
    "\n",
    "# Grid search\n",
    "lin_grid_reg = GridSearchCV(lin_reg, param_grid, cv=3)\n",
    "# Fit model on training data\n",
    "lin_grid_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get best model by mean test score\n",
    "best_index = np.argmax(lin_grid_reg.cv_results_[\"mean_test_score\"])\n",
    "# Get best predictions by predicting on validation set\n",
    "best_lin_pred = lin_grid_reg.best_estimator_.predict(X_validate)\n",
    "\n",
    "# Print the best parameters, CV r2, validation r2, and validation RMSE\n",
    "print(lin_grid_reg.cv_results_[\"params\"][best_index])\n",
    "print('Best CV R^2:', max(lin_grid_reg.cv_results_[\"mean_test_score\"]))\n",
    "print('Validation R^2:', lin_grid_reg.score(X_validate, y_validate))\n",
    "print('Validation RMSE', rmse(best_lin_pred, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a grid search for both Ridge and LASSO. In particular, make sure to search across a variety of alpha values in addition to varying other hyperparameters.\n",
    "\n",
    "*Recall: `GridSearchCV` takes two arguments: a model and a dict of parameter variations (`param_grid`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "param_grid = {'alpha': ...,\n",
    "               ...: ...,\n",
    "             ...: ...,\n",
    "             ...: ...}\n",
    "\n",
    "# Grid search\n",
    "ridge_grid_reg = GridSearchCV(..., ..., cv=3)\n",
    "# Fit model on training data\n",
    "...\n",
    "\n",
    "# Get best model by mean test score\n",
    "best_index = ...\n",
    "# Get best predictions by predicting on validation set\n",
    "best_ridge_pred = ...\n",
    "\n",
    "# Print the best parameters, CV r2, validation r2, and validation RMSE\n",
    "...\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "param_grid = {...}\n",
    "\n",
    "# Grid search\n",
    "lasso_grid_reg = GridSearchCV(..., ..., cv=3)\n",
    "# Fit model on training data\n",
    "...\n",
    "\n",
    "# Get best model by mean test score\n",
    "best_index = ...\n",
    "# Get best predictions by predicting on validation set\n",
    "best_lasso_pred = ...\n",
    "\n",
    "# Print the best parameters, CV r2, validation r2, and validation RMSE\n",
    "...\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Choosing a model <a id='section_6'></a>\n",
    "---\n",
    "### Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your best model with the best hyperparameter values and try it on the test set. How well does it do?\n",
    "\n",
    "Make sure to look at the documentation for [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and see which methods are available.\n",
    "\n",
    "*Tip: first, apply the `.best_estimator_.predict()` method to one of your GridSearchCV objects--`lin_grid_reg`, `ridge_grid_reg`, or `lasso_grid_reg`--depending on which one was most succesful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_pred = ...\n",
    "print('Best CV R^2:', max(....cv_results_[\"mean_test_score\"]))\n",
    "print('Test R^2:', ....score(X_test, y_test))\n",
    "print('Test RMSE', rmse(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the RMSEs for the validation data compare to those for the training data? Why?\n",
    "\n",
    "Did the model that performed best on the training set also do best on the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, select one final model to make predictions for your test set. This is often the model that performed best on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the test set using one model of your choice\n",
    "final_pred = ...\n",
    "# calculate the rmse for the final predictions\n",
    "print('Test set rmse: ', rmse(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming up this semester: how to select your models, model parameters, and features to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Materials borrowed from notebook developed by Keeley Takimoto for LS123: Data, Prediction, and Law\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
