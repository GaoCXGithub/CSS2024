{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 4-1 Clustering and PCA - Student Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsuperivsed ML method used to group data points based on their features alone, and no observed grouping labels as in supervised classification. Thus most clustering alorithms seeks to group points by their distance in a high dimensional space generated by provided features.\n",
    "\n",
    "Below is a plot showing the results of the clustering algorithms in Scikit-Learn for several different toy datasets.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This prevents the potential of crashing your root Python/Anaconda installation. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. If you have already created a virtual enviornment, you can run the following command to activate it. <br>\n",
    "`conda activate <virtual_env_name>`\n",
    "For example, if your virtual environment was named as legal-studies, run the following command. <br>\n",
    "`conda activate legal-studies`\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. <br>\n",
    "`conda deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) K-means clustering  \n",
    "\n",
    "In this section we will cover k-means clustering using `scikit-learn`. The scikit-learn documentation for clustering is found [here](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "First we'll import `KMeans` and `numpy` so that we can make our arrays. The `%matplotlib inline` will make our plots show up within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a few points arranged in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,1], [1,2], [1, 0], [-1, -3],\n",
    "             [15, 21], [18, 30], [20, 20], [22, 19],\n",
    "             [45, 50], [42, 48], [60, 40], [50, 50]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot them we can see that they appear to be arranged roughly in three groups. *Note: the asterisk is used for \"unpacking\" the two lists in `X.T` into the function call of `plt.scatter`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X.T)\n",
    "plt.title('random points')\n",
    "plt.xlabel('feature0')\n",
    "plt.ylabel('feature1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our clusters, all we have to do is specify how many we want, and then fit the model to the data. We'll choose 3. We can also specify the maximum number of iterations of the k-means algorithm, which you may want to do with a much larger dataset.\n",
    "\n",
    "First thing's first: **set a random seed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the model. We'll use the [`KMeans()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) methods from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3,\n",
    "               max_iter=300 #default\n",
    "               ).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the centers of the clusters through the `cluster_centers_` attribute. To get the labels (i.e. the corresponding cluster) we use `labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Centers\")\n",
    "print(kmeans.cluster_centers_)\n",
    "print()\n",
    "\n",
    "print(\"Labels\")\n",
    "print(kmeans.labels_)\n",
    "print()\n",
    "\n",
    "for point, label in zip(X, kmeans.labels_):\n",
    "    print(\"Coordinates:\", point, \"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also plot out cluster centers along with the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*X[kmeans.labels_==0,:].T, s=50, c='r', label='Cluster 0')\n",
    "ax1.scatter(*X[kmeans.labels_==1,:].T, s=50, c='b', label='Cluster 1')\n",
    "ax1.scatter(*X[kmeans.labels_==2,:].T, s=50, c='g', label='Cluster 2')\n",
    "ax1.scatter(*kmeans.cluster_centers_.T, s=50, marker='+', c='black', label='cluster centers')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('feature0')\n",
    "plt.ylabel('feature1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see to which cluster a new point would belong, we simply use the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_points = np.asarray([[0, 4],\n",
    "                        [19, 25],\n",
    "                        [40, 50]])\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print()\n",
    "\n",
    "print(\"0, 4\")\n",
    "print(\"Cluster:\", kmeans.predict([[0, 4]]))\n",
    "print()\n",
    "\n",
    "print(\"19, 25\")\n",
    "print(\"Cluster:\", kmeans.predict([[19, 25]]))\n",
    "print()\n",
    "\n",
    "print(\"40, 50\")\n",
    "print(\"Cluster:\", kmeans.predict([[40, 50]]))\n",
    "\n",
    "#plot new points\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*X[kmeans.labels_==0,:].T, s=50, c='r', label='Cluster 0')\n",
    "ax1.scatter(*X[kmeans.labels_==1,:].T, s=50, c='b', label='Cluster 1')\n",
    "ax1.scatter(*X[kmeans.labels_==2,:].T, s=50, c='g', label='Cluster 2')\n",
    "ax1.scatter(*kmeans.cluster_centers_.T, s=50, c='black', marker='+', label='cluster centers')\n",
    "ax1.scatter(*new_points.T, s=50, c='cyan', label='new points')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('feature0')\n",
    "plt.ylabel('feature1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll show an example of agglomerative clustering, which is a type of hierarchical clustering. The documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) in case you want to know more about the parameters. We'll use some of scikitlearn's toy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 1500\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)[0]\n",
    "blobs, blob_truth = datasets.make_blobs(n_samples=n_samples, random_state=0)\n",
    "\n",
    "plt.scatter(*noisy_moons.T)\n",
    "plt.ylabel('noisy moons')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(*blobs.T)\n",
    "plt.ylabel('blobs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use two clusters this time, and use ward linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ward = AgglomerativeClustering(n_clusters=3,\n",
    "                               linkage='ward', #linkage can be ward (default), complete, or average\n",
    "                               affinity='euclidean') #affinity must be euclidean if linkage=ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit the clustering model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.fit(noisy_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll sort the points by label and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 0])\n",
    "one = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 1])\n",
    "two = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 2])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "ax1.scatter(*two.T, s=50, c='g', label='two')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same with the blobs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.fit(blobs)\n",
    "\n",
    "zero = np.array([point for label, point in zip(ward.labels_, blobs) if label == 0])\n",
    "one = np.array([point for label, point in zip(ward.labels_, blobs) if label == 1])\n",
    "two = np.array([point for label, point in zip(ward.labels_, blobs) if label == 2])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "ax1.scatter(*two.T, s=50, c='g', label='two')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: DBSCAN \n",
    "\n",
    "\n",
    "It looks like our agglomerative clustering model did not cluster the noisy moons dataset how we might have wanted. For the challenge, use [`DBSCAN`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) to cluster noisy moons. Then plot the results and see what it looks like. Try an `eps` value of .2. This sets the maximum distance between two samples for them to be considered in the same neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model object\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# define model object\n",
    "dbscan = ...\n",
    "\n",
    "# fit model to data \n",
    "...;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fitted labels for each data point \n",
    "labels = ...\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there any outliers not included in either cluster (indicated with a `-1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inferred clusters\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into those for each cluster \n",
    "zero = np.array([point for label, point in zip(..., noisy_moons) if label == 0])\n",
    "one = np.array([point for label, point in zip(..., noisy_moons) if label == 1])\n",
    "\n",
    "# plot data with cluster assignment as the color \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit another DBSCAN model to the blobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model object\n",
    "dbscan = ...\n",
    "\n",
    "# fit model to data \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ...\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, see if there are outliers not included in any cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inferred clusters\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot the points in the blobs dataset, coloring them by their cluster id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(blobs[:, ... ],blobs[:, ...], s=50, c=labels, label='zero')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is an unsupervised machine learning technique. At a basic level, it summarizes information in many features by collapsing them into fewer features. PCA can be used for both exploratory data analysis and dimensionality reduction. For this exercise, we are going to use the [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) from sklearn. First, let's load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load data\n",
    "breast = load_breast_cancer()\n",
    "# Features array\n",
    "breast_data = breast.data\n",
    "# Target array\n",
    "breast_labels = breast.target\n",
    "# Reshape target array\n",
    "labels = np.reshape(breast_labels,(569,1))\n",
    "# Concatenate features and labels\n",
    "final_breast_data = np.concatenate([breast_data,labels],axis=1)\n",
    "# Get feature names\n",
    "features = breast.feature_names\n",
    "features_labels = np.append(features,'label')\n",
    "# Coerce to dataframe and add column names\n",
    "breast_dataset = pd.DataFrame(final_breast_data)\n",
    "breast_dataset.columns = features_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out first 5 rows\n",
    "breast_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the \"label\" column that will be our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recode 0 to \"benign\" and 1 to \"malignant\" to make these more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_dataset['label'].replace(0, 'Benign', inplace = True)\n",
    "breast_dataset['label'].replace(1, 'Malignant', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with supervised methods, scaling our data in advance is usually a good idea. Apply the sklearn [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to the features in our dataframe and save the result as an array called \"X\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = breast_dataset.loc[:, features].values\n",
    "X = StandardScaler().fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our PCA! Use the [`PCA()`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method from sklearn to perform a PCA on the breast cancer features and summarize them with two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents_breast = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(data = principalComponents_breast\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Principal Component - 1',fontsize=20)\n",
    "plt.ylabel('Principal Component - 2',fontsize=20)\n",
    "plt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\n",
    "targets = ['Benign', 'Malignant']\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = breast_dataset['label'] == target\n",
    "    plt.scatter(pca_df.loc[indicesToKeep, 'principal component 1']\n",
    "               , pca_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
    "\n",
    "plt.legend(targets,prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression that predicts the label using all of the features. Then train a second logistic regression model that uses only the principal components as features. How do the confusion matrices compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Target\n",
    "#lb_style = LabelBinarizer()\n",
    "y = ...\n",
    "\n",
    "# Features\n",
    "X_original = ...\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_original, y, train_size = .80, test_size=0.20)\n",
    "\n",
    "# create a model\n",
    "logit_reg = LogisticRegression(max_iter= 5000) \n",
    "\n",
    "# fit the model\n",
    "logit_model = logit_reg.fit(..., ...)\n",
    "\n",
    "y_pred = logit_model.predict(...)\n",
    "\n",
    "cf_matrix = confusion_matrix(..., ...)\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Benign\", 1: \"Malignant\"})\n",
    "df_cm.index = [\"Benign\", \"Malignant\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With PCA Features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Target\n",
    "#lb_style = LabelBinarizer()\n",
    "y = ...\n",
    "\n",
    "# Features\n",
    "X = ...\n",
    "X = ...\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .80, test_size=0.20)\n",
    "\n",
    "# create a model\n",
    "logit_reg = LogisticRegression(max_iter= 5000) \n",
    "\n",
    "# fit the model\n",
    "logit_model = logit_reg.fit(..., ...)\n",
    "\n",
    "y_pred = logit_model.predict(...)\n",
    "\n",
    "cf_matrix = confusion_matrix(..., ...)\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Benign\", 1: \"Malignant\"})\n",
    "df_cm.index = [\"Benign\", \"Malignant\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How did the logistic regression trained on just the PCA features compare to the original?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Materials borrowed from D-Lab's [Python Machine Learning Workshop](https://github.com/dlab-berkeley/python-machine-learning/blob/master/3_clustering.ipynb), and [datacamp](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python). "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
