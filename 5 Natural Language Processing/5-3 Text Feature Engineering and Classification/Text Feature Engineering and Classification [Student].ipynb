{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 5-3 Text Feature Engineering and Classification - Student Version\n",
    "\n",
    "In this lab we will use the techniques we covered so far to engineer text features and train a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../images/cfpb logo.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again use the Consumer Financial Protection Bureau's [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). Picking up from where we left off last time, we'll focus on predicting whether a consumer complaint narrative is talking about a \"checkings or savings account\" issue or a \"student loan\" issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative'])\n",
    "cfpb = cfpb[(cfpb['Product']=='Checking or savings account') | (cfpb['Product'] == 'Student loan')]\n",
    "cfpb = cfpb[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any feature engineering or classification, we should first preprocess our text. Let's start by defining our custom `rem_punc_stop()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc_stop(text):\n",
    "    stop_words = STOP_WORDS\n",
    "    # Individually\n",
    "    # nlp.Defaults.stop_words.add(\"XX\")\n",
    "    # nlp.Defaults.stop_words.add(\"XXXX\")\n",
    "    # nlp.Defaults.stop_words.add(\"XXXXXXX\")\n",
    "    \n",
    "    # Using the bitwise |= (or) operator\n",
    "    nlp.Defaults.stop_words |= {\"XX\", \"XXXX\",\"XXXXXXXX\"}\n",
    "    \n",
    "    punc = set(punctuation)\n",
    "    \n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "    \n",
    "    doc = nlp(punc_free)\n",
    "    \n",
    "    spacy_words = [token.text for token in doc]\n",
    "    \n",
    "    no_punc = [word for word in spacy_words if word not in stop_words]\n",
    "    \n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and apply our function to the consumer complaint narratives. Notice how the `rem_punc_stop()` function returns a list, but we can collapse our tokens back into strings with the `join()` string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb['tokens'] = cfpb['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
    "cfpb['tokens'] = cfpb['tokens'].map(lambda text: ' '.join(text))\n",
    "cfpb['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already explored several exploratory data analysis techniques. There are many different ways to explore text data that we haven't covered, but let's take a look at one last basic tool: visualizing n-grams.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BOW countervectorizer\n",
    "## Notice the ngram_range argument\n",
    "countvec = CountVectorizer(stop_words=STOP_WORDS, ngram_range=(2,3))\n",
    "ngrams = countvec.fit_transform(cfpb['tokens'])\n",
    "\n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(), columns = countvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0:'freq'})\n",
    "df_ngram = df_ngram.sort_values(by = ['freq'], ascending = False).reset_index(drop = True)\n",
    "df_ngram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"ngrams\", y = 'freq', data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the code above to visualize the most popular unigrams and 4-grams. What is the tradeoff involved with increasing n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-gram\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we saw some techniques for exploring the text of our data. Specifically, we saw how to find the length of our text and word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb['complaint_len'] = cfpb['tokens'].apply(len)\n",
    "cfpb['complaint_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb['word_count'] = cfpb['tokens'].apply(lambda x: len(str(x).split()))\n",
    "cfpb['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also covered subjectivity and sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb['polarity'] = cfpb['tokens'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "cfpb['subjectivity'] = cfpb['tokens'].map(lambda text: TextBlob(text).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's take the top 25 n-grams we found earlier and turn them into their own dataframe. We'll return to these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words=STOP_WORDS, ngram_range=(2,3))\n",
    "ngrams = countvec.fit_transform(cfpb['tokens'])\n",
    "\n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(), columns = countvec.get_feature_names())\n",
    "\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0:'freq'})\n",
    "df_ngram = df_ngram.sort_values(by = ['freq'], ascending = False).reset_index(drop = True)\n",
    "\n",
    "top_25_ngrams = dictionary_dataframe.loc[:,df_ngram[0:25]['ngrams']]\n",
    "top_25_ngrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code we went over from the last lab, make a dataframe with 5 LDA generated topics. Then, create a topic model using [Non-Negative Matrix Factorization](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) and print out the words associated with the first 5 topics. NMF is another algorithm that is frequently used for topic modeling. Do you get similar topics as with your LDA topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to move to classification! We are going to examine how different featurization techniques compare. Create a list with the following:\n",
    " * Text Engineered Features\n",
    " * Text Engineered Features + Topic Model\n",
    " * Non-Text Features only\n",
    " * Non-Text Features + Text Engineered Features\n",
    " * Tf-idf\n",
    " * Non-Text Features + tf-idf\n",
    " * Non-Text Features + Top 25 n-gram\n",
    "\n",
    "You'll need to use pandas to create and .`join()` these different dataframes together. Also be sure to use `reset_index()` as necessary. Once you've created each of these dataframes (or arrays!) you should loop through all of them, train a supervised learning algorithm (like logistic regression or a decision tree classifier), and plot confusion matrices. Once you do this, think about which featurization technique worked the best, and whether combining text and non-text features was helpful. For now, don't worry about hyperparameter tuning or feature selection, though you would do these in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineered Text Features\n",
    "engineered_features = ...\n",
    "\n",
    "# Topic Model + Engineered Features\n",
    "engineered_features_with_topics = ...\n",
    "\n",
    "# Non-text features\n",
    "# Hint: Is there something we need to do to prepare categorical features for classification?\n",
    "non_text_features = ...\n",
    "non_text_features_dummies = ...\n",
    "\n",
    "# Non-text features + engineered features\n",
    "non_text_engineered_features = ...\n",
    "\n",
    "# Non-text features + tfidf\n",
    "tfidf_df = ...\n",
    "non_text_plus_tfidf = ...\n",
    "\n",
    "# Top 25 ngrams + non-text\n",
    "non_text_with_ngrams = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [...]\n",
    "\n",
    "featurization_technique = [...]\n",
    "\n",
    "# Hint: Is there something we need to do to y to prepare it for classification?\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe, featurization in zip(dataframes, featurization_technique):\n",
    "   \n",
    "    # The code to plot a confusion matrix is provided in the for loop - fill in the code you would need to create the confusion matrix before this\n",
    "    ...\n",
    "\n",
    "    df_cm = df_cm.rename(index=str, columns={0: \"Checking or savings account\", 1: \"Student loan\"})\n",
    "    df_cm.index = [\"Checking or savings account\", \"Student loan\"]\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    sns.heatmap(df_cm, \n",
    "               annot=True,\n",
    "               annot_kws={\"size\": 16},\n",
    "               fmt='g')\n",
    "\n",
    "    plt.title(featurization)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, combing text with non-text features will improve a classifier's performance. However this isn't automatic - in some cases you can actually degrade a classifier's performance by adding in more features. In this case, our engineered features were too quick to predict \"checking or savings account\" and our tf-idf alone outperformed tf-idf + non-text features. However, non-text features + n-grams was tied with tf-idf alone! We might prefer the former approach because it is computationally cheaper, and likely easier to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by Aniket Kesari"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
