{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 5-1 Text Preprocessing and Featurization - Student Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover the basics of text preprocessing and featurization, and introduce text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. If you have already created a virtual enviornment, you can run the following command to activate it. <br>\n",
    "`conda activate <virtual_env_name>`\n",
    "For example, if your virtual environment was named `css_lab`, run the following command. <br>\n",
    "`conda activate css_lab` <br>\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. <br>\n",
    "`conda deactivate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../images/cfpb logo.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next few labs, we will use the Consumer Financial Protection Bureau's [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). The database is rich with information about individual consumer complaints about credit card fraud, debt collections, and other consumer issues. This dataset is convenient for text analysis because the consumer complaints are real text generated by real people - and have all the idiosyncrasies that come with that process. It also contains multiple different categories that we can predict, like type of product the complaint is about and whether the complaint was resolved quickly. The basic process is that if someone has a dispute related to consumer finance (mortgages, student loans, credit cards, etc.), they can file a dispute with the CFPB, which then contacts the company named in the dispute to get some resolution of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>01/30/20</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a credit reporting company's inve...</td>\n",
       "      <td>Investigation took more than 30 days</td>\n",
       "      <td>Reviewed my credit report in XX/XX/XXXX and no...</td>\n",
       "      <td>None</td>\n",
       "      <td>EQUIFAX, INC.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>850XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>01/30/20</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3515096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>03/12/20</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Account status incorrect</td>\n",
       "      <td>TransUnion has not properly investigated the i...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>IL</td>\n",
       "      <td>606XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>03/12/20</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3564439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>05/01/20</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a credit reporting company's inve...</td>\n",
       "      <td>Their investigation did not fix an error on yo...</td>\n",
       "      <td>XX/XX/2020 someone tried to steal my identity ...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>IL</td>\n",
       "      <td>606XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>05/01/20</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3633318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>04/06/20</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Other debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>I paid the debt on XX/XX/XXXX. I disputed acco...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>Experian Information Solutions Inc.</td>\n",
       "      <td>NY</td>\n",
       "      <td>None</td>\n",
       "      <td>Servicemember</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>04/06/20</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3594679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>04/18/20</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I do not know</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>A COLLECTION HAS BEEN REPORTED TO MY CREDIT RE...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>Convergent Resources, Inc.</td>\n",
       "      <td>FL</td>\n",
       "      <td>336XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>04/18/20</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3611900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Date received                                            Product  \\\n",
       "0      8      01/30/20  Credit reporting, credit repair services, or o...   \n",
       "1     10      03/12/20  Credit reporting, credit repair services, or o...   \n",
       "2     14      05/01/20  Credit reporting, credit repair services, or o...   \n",
       "3     19      04/06/20                                    Debt collection   \n",
       "4     35      04/18/20                                    Debt collection   \n",
       "\n",
       "        Sub-product                                              Issue  \\\n",
       "0  Credit reporting  Problem with a credit reporting company's inve...   \n",
       "1  Credit reporting               Incorrect information on your report   \n",
       "2  Credit reporting  Problem with a credit reporting company's inve...   \n",
       "3        Other debt                  Attempts to collect debt not owed   \n",
       "4     I do not know                  Attempts to collect debt not owed   \n",
       "\n",
       "                                           Sub-issue  \\\n",
       "0               Investigation took more than 30 days   \n",
       "1                           Account status incorrect   \n",
       "2  Their investigation did not fix an error on yo...   \n",
       "3                                      Debt was paid   \n",
       "4                                  Debt is not yours   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  Reviewed my credit report in XX/XX/XXXX and no...   \n",
       "1  TransUnion has not properly investigated the i...   \n",
       "2  XX/XX/2020 someone tried to steal my identity ...   \n",
       "3  I paid the debt on XX/XX/XXXX. I disputed acco...   \n",
       "4  A COLLECTION HAS BEEN REPORTED TO MY CREDIT RE...   \n",
       "\n",
       "                             Company public response  \\\n",
       "0                                               None   \n",
       "1  Company has responded to the consumer and the ...   \n",
       "2  Company has responded to the consumer and the ...   \n",
       "3  Company has responded to the consumer and the ...   \n",
       "4  Company has responded to the consumer and the ...   \n",
       "\n",
       "                                  Company State ZIP code           Tags  \\\n",
       "0                           EQUIFAX, INC.    AZ    850XX           None   \n",
       "1  TRANSUNION INTERMEDIATE HOLDINGS, INC.    IL    606XX           None   \n",
       "2  TRANSUNION INTERMEDIATE HOLDINGS, INC.    IL    606XX           None   \n",
       "3     Experian Information Solutions Inc.    NY     None  Servicemember   \n",
       "4              Convergent Resources, Inc.    FL    336XX           None   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0           Consent provided           Web             01/30/20   \n",
       "1           Consent provided           Web             03/12/20   \n",
       "2           Consent provided           Web             05/01/20   \n",
       "3           Consent provided           Web             04/06/20   \n",
       "4           Consent provided           Web             04/18/20   \n",
       "\n",
       "      Company response to consumer Timely response?  Consumer disputed?  \\\n",
       "0          Closed with explanation              Yes                 NaN   \n",
       "1          Closed with explanation              Yes                 NaN   \n",
       "2  Closed with non-monetary relief              Yes                 NaN   \n",
       "3          Closed with explanation              Yes                 NaN   \n",
       "4          Closed with explanation              Yes                 NaN   \n",
       "\n",
       "   Complaint ID  \n",
       "0       3515096  \n",
       "1       3564439  \n",
       "2       3633318  \n",
       "3       3594679  \n",
       "4       3611900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfpb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Reviewed my credit report in XX/XX/XXXX and no...\n",
       "1    TransUnion has not properly investigated the i...\n",
       "2    XX/XX/2020 someone tried to steal my identity ...\n",
       "3    I paid the debt on XX/XX/XXXX. I disputed acco...\n",
       "4    A COLLECTION HAS BEEN REPORTED TO MY CREDIT RE...\n",
       "Name: Consumer complaint narrative, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfpb['Consumer complaint narrative'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the **process of splitting text into words and sentences.** These chunks (words, sentences, etc.) are called **tokens**. One approach might be to try to do this use string methods like [str.split](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html). The problem with this is that using a separator like a \",\" or \".\" or \" \" may not work for some common situations. So instead, we'll use the [spaCy](https://spacy.io/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why tokenize?\n",
    "\n",
    "Electronic text is a linear sequence of symbols. Before any processing can be done, text needs to be segmented into linguistic units, and this process is called tokenization.\n",
    "\n",
    "We usually look at grammar and meaning at the level of words, related to each other within sentences, within each document. So if we're starting with raw text, we first need to split the text into sentences, and those sentences into words -- which we call \"tokens\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tokenize\n",
    "\n",
    "#### Using String Methods\n",
    "\n",
    "##### Split Into Sentences\n",
    "\n",
    "You might imagine that the easiest way to identify sentences is to split the document at every period '.', and to split the sentences using white space to get the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the split function to create tokens\n",
    "text = cfpb['Consumer complaint narrative'][0]\n",
    "paragraph = ...\n",
    "sentences = paragraph.split(...)\n",
    "for s in sentences[:5]:\n",
    "    print(s + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be ok, but what if someone said something like \"U.C. Berkeley charged me $50.11 by mistake.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_text = \"U.C. Berkeley charged me $50.11 by mistake.\"\n",
    "bad_sentences = bad_text.split(\".\")\n",
    "for s in bad_sentences[:5]:\n",
    "    print(s + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look too good! The one sentence was split into 4 separate sentences because \".\"'s are used for things other than ending a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Into Tokens\n",
    "From here, we might split each sentence into tokens by splitting on white space in between words. Try filling in the code below to take the first sentence and split on white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ...\n",
    "tokens = sentence.split(...)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 1: What was the problem with splitting on the white space? Are there any tokens that look a little strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to string methods, spaCy uses pre-trained language models to incorporate context. In this case, we'll load the [en_core_web_sm](https://spacy.io/models/en), which is one of spaCy's English language models. For instance, the end of a sentence (\".\") should mark a new token, but the string \"U.K.\" should not be separated at the \".\"'s. According to [spaCy's documentation](https://spacy.io/usage/spacy-101#annotations-token) it achieves this by taking the following steps:\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try applying these methods to our CFPB data. The steps to do this are:\n",
    "\n",
    "1. Load the language model.\n",
    "2. Apply it to a piece of text and save it in an spaCy \"doc\" object.\n",
    "3. Extract each token from the doc object to a list.\n",
    "4. Display the tokens\n",
    "\n",
    "Check the documentation for help filling in these steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(...)\n",
    "spacy_words = [... for ... in ...]\n",
    "display(f\"Tokenized words: {spacy_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words and Punctuation\n",
    "\n",
    "We now have some tokens with just a few lines of code! There are a few additional steps that we might want to take. For example, we may want to remove punctuation and stop words. Punctuation oftentimes does not add substantive information to a piece of text, and stop words are common words that appear very frequently across texts. Removing this kind of information can help with downstream classification tasks by allowing an algorithm to focus on words that distinguish documents, rather than ones that appear frequently across them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at stop words. We can start by importing a collection of stop words from spaCy by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some common stop words from this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(STOP_WORDS)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation and stop words is not a hard and fast rule - there may be situations where you want to keep them. In most applications, they add noise to downstream tasks, but always be mindful of your particular application when making decisions. Now that we have some tokenization tools, let's put them all together in a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 2: Write a function that takes a piece of text as an argument, and returns a list of tokens without punctuation or stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_reduced = []\n",
    "\n",
    "def rem_punc_stop(text):\n",
    "    stop_words = ...\n",
    "    punc = set(...)\n",
    "    \n",
    "    punc_free = \"\".join([... for ... in ... if ... not in ...])\n",
    "    \n",
    "    doc = nlp(...)\n",
    "    \n",
    "    spacy_words = [...]\n",
    "    \n",
    "    no_punc = [...]\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_reduced = rem_punc_stop(text)\n",
    "tokens_reduced[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also contains a number of methods for things like entity recognition. For instance, we could run the following code to check various entities. Notice that this process isn't perfect, spaCy still thinks \"XX/XX/XXXX\" is an organization or product even though we know this is a redacted date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in nlp(text).ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preprocessing step we might take is reducing words down to their lemmas. Lemmatization reduces a word to its root word, while making sure the word still belongs to the language. This is in contrast to stemming, which reduces the word down to its root even if that root is not a valid word. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp(u'compute computer computed computing'):\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming these words would all result in the root \"comput\" but lemmatization converted these words to their shortest variant. Again, you may choose to stem or lemmatize depending on your specific application.\n",
    "\n",
    "##### Challenge 3: Lemmatize the first consumer complaint narrative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ... in nlp(...):\n",
    "    print(...,  ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to chunk more than one word together. One way to do this might be to group nounds together. \n",
    "\n",
    "**Challenge 4: Trying using the [`noun_chunks`](https://spacy.io/api/doc#noun_chunks) method to chunk nouns in the first complaint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(...)\n",
    "for np in ...:\n",
    "    print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have covered some the basics of text preprocessing, we are ready to start getting our data in a format for feeding it into machine learning algorithms. There are many options for converting raw text to features in a supervised machine learning problem. The most basic of these is the \"bag of words\" approach. Bag of words essentially counts the number of times each word appears in a corpus, and these counts become features.\n",
    "\n",
    "To illustrate, first let's import the CounterVectorizer method from sklearn. Once we do that, let's use our tokenizer function that we wrote earlier to initialize the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = rem_punc_stop, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create a CountVectorizer object, we can then transform a list of texts with the \"fit_transform\" method. This will return a sparse matrix with the counts. We can densify the matrix with the \".todense()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = bow_vector.fit_transform(cfpb['Consumer complaint narrative'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = bow_vector.get_feature_names()\n",
    "feature_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extension of bag-of-words is the term frequency-inverse document frequency approach. Whereas bag-of-words counts the number of words in the document. tf-idf takes this quanity and divides it by how frequently the word shows up across the corpus. In doing so, the tf-idf score downweights words that are common in the corpus and thus would not aid with classification.\n",
    "\n",
    "**Challenge 5: Using the code from the \"Bag of Words\" section as a template, write code to get the tf-idf matrix for the CFPB data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = ...\n",
    "feature_names = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have featurized our text, we are ready to make a prediction! Does the text of our consumer complaints predict whether or not they get a timely response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 6: Transform the text of the consumer complaint narrative into a tf-idf matrix, and use it to predict the \"Timely response?\" column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb = cfpb[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation, Test Sets\n",
    "\n",
    "# X\n",
    "X = ...\n",
    "tf = ...\n",
    "\n",
    "tfidf_matrix =  ...\n",
    "\n",
    "#y\n",
    "\n",
    "y = ...\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Train/Validation Split\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb_model = nb.fit(..., ...)\n",
    "\n",
    "nb_pred = nb_model.predict(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(... == ...)\n",
    "\n",
    "nb_cf_matrix = confusion_matrix(..., ...)\n",
    "nb_cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix! Use the following code from the \"seaborn\" package to make a heatmap out of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df_cm = pd.DataFrame(nb_cf_matrix, range(2),\n",
    "                  range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df_cm = nb_df_cm.rename(index=str, columns={0: \"no\", 1: \"yes\"})\n",
    "nb_df_cm.index = [\"no\", \"yes\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(nb_df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16})\n",
    "\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
