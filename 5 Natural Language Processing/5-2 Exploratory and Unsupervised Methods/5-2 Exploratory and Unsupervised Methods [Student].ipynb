{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 5-2 Exploratory Data Analysis and Unsupervised Methods - Student Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will demonstrate some exploratory methods for finding separating words, and introduce unsupervised topic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download libraries\n",
    "# ----------\n",
    "#!pip install scattertext\n",
    "#!pip install wordcloud\n",
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import scattertext as st\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# settings \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../images/cfpb_logo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again use the Consumer Financial Protection Bureau's [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). This time, we are going to focus on figuring out whether we can find text features that help distinguish different \"Products.\" There are several products represented in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# ----------\n",
    "\n",
    "# load the dataframe\n",
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "\n",
    "# drop missing on \"Consumer complaint narrative\" feature and reset the index bc we've dropped\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative'])# not reseting the index here bc doing some other cleaning\n",
    "\n",
    "# identify the unique characters is Product column\n",
    "cfpb['Product'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first few exercises, we will focus on mortgages and student loans. We will also just use the first one thousand observations so that the code runs faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out only columns where product == \"Mortgage\" or \"Student loan\"\n",
    "cfpb = cfpb[(cfpb['Product']=='Mortgage') | (cfpb['Product'] == 'Student loan')]\n",
    "\n",
    "# subset the first 1000 rows \n",
    "cfpb = cfpb[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view \n",
    "# ----------\n",
    "cfpb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating our tokens. We'll use the same `rem_punc_stop()` function we defined last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing function - just like we did in the last lab\n",
    "# ----------\n",
    "def rem_punc_stop(text):\n",
    "\n",
    "    # set objects\n",
    "    stop_words = STOP_WORDS         # set STOP_WORDS to a new object variable\n",
    "    punc = set(punctuation)         # convert punctuation to a set\n",
    "    \n",
    "    # essentially remove the punctuation - important to remove punctuation first to correctly capture stop words\n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc]) # join new list of characters (ch) in text w/ condition\n",
    "                                                               # if ch is not in punctuation \n",
    "                                                               # \"\".join() creates a string from the list comprehension\n",
    "\n",
    "    # apply nlp to punctuation-free object\n",
    "    doc = nlp(punc_free)\n",
    "    \n",
    "    # extract words from processed text \n",
    "    spacy_words = [token.text for token in doc]\n",
    "    \n",
    "    # filter out words \n",
    "    no_punc = [word for word in spacy_words if word not in stop_words]\n",
    "    \n",
    "    # return\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here how we use the `map()` function to apply our `rem_punc_stop()` function to every row of our dataframe. `map()` is typically much faster than writing a for loop, though there are also faster options like [list comprehensions](https://docs.python.org/3/tutorial/datastructures.html) and vectorized numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply the function to all the columns in our dataframe\n",
    "# ----------\n",
    "cfpb['tokens'] = cfpb['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x)) # can use apply here \n",
    "cfpb['tokens'] # visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular text analysis visualizations is the word cloud. Word clouds visualize the most frequent words in a corpus, and size them according to frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply the function to all the columns in our dataframe\n",
    "# ----------\n",
    "# apply function to text object\n",
    "text = ' '.join(cfpb['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "wordcloud = WordCloud(random_state=40)...\n",
    "\n",
    "# plot \n",
    "plt.imshow(wordcloud,                  # specify wordcloud\n",
    "           interpolation = 'bilinear') # specifies how the words are displayed\n",
    "plt.axis('off')                        # turn off axes\n",
    "plt.show()                             # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of ways to customize a word cloud, including by changing the background color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same word cloud as above but changing background parameters\n",
    "# ----------\n",
    "# apply function to text object\n",
    "text = ' '.join(cfpb['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "wordcloud = WordCloud(...,             # set background color to white\n",
    "                      random_state=41  # set random state to ensure same word cloud each time\n",
    "                      )...)            # change the background color\n",
    "\n",
    "\n",
    "\n",
    "# plot \n",
    "plt.imshow(wordcloud,                  # specify wordcloud\n",
    "           interpolation = 'bilinear') # specifies how the words are displayed \n",
    "plt.axis('off')                        # turn off axes\n",
    "plt.show()                             # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can even overlay the wordcloud onto an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread the word cloud over the CFPB logo\n",
    "# ----------\n",
    "# load image\n",
    "cfpb_mask = np.array(Image.open(\"../../images/cfpb_logo.png\"))\n",
    "\n",
    "# create word cloud\n",
    "text = ' '.join(cfpb['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# specify parameters of the wordcloud\n",
    "wordcloud = WordCloud(...                        # set background color to white\n",
    "                      random_state=77,           # set random state to ensure same word cloud each time\n",
    "                      mask = cfpb_mask,          # mask is necessary \n",
    "                      contour_width=0.0001,      # provides an outline for clarity\n",
    "                      width = 1000,\n",
    "                      height = 1000).generate(text)\n",
    "\n",
    "# plot \n",
    "plt.imshow(wordcloud,                   # specify wordcloud\n",
    "           interpolation = 'bilinear')  # specifies how the words are displayed\n",
    "plt.axis('off')                         # turn off axes\n",
    "plt.show()                              # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Notice that in the above word clouds, tokens like \"XXXX\" and \"XXXXXXXX\" appear frequently. These are redacted dates and likely won't help us with classification. Try to rewrite `rem_punc_stop` to remove these.\n",
    "\n",
    "**Hint**: Try taking a look at `nlp.Defaults.stop_words` and see if there are any associated methods that might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite the rem_punc_stop function to remove redacted account numbers (e.g, \"XXX\")\n",
    "# ----------\n",
    "\n",
    "def rem_punc_stop(text):\n",
    "    stop_words = STOP_WORDS\n",
    "\n",
    "    # remove stop words\n",
    "    nlp.Defaults.stop_words ...\n",
    "    \n",
    "    # convert punctuation to set\n",
    "    punc = set(...)\n",
    "    \n",
    "    # essentially remove the punctuation - important to remove punctuation first to correctly capture stop words\n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc]) # join new list of characters (ch) in text w/ condition\n",
    "                                                               # if ch is not in punctuation \n",
    "                                                               # \"\".join() creates a string from the list comprehension\n",
    "\n",
    "    # apply nlp to punctuation-free object\n",
    "    doc = nlp(...)\n",
    "    \n",
    "    # extract words from processed text \n",
    "    spacy_words = [token.text for token in doc]\n",
    "    \n",
    "    # filter out words \n",
    "    no_punc = [word for word in spacy_words if word not in stop_words]\n",
    "    \n",
    "    # return\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate word cloud from above \n",
    "# ----------\n",
    "\n",
    "# need to do preprocessing again and re-create \"tokens\" column\n",
    "cfpb['tokens'] = cfpb['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
    "\n",
    "# apply function to text object\n",
    "text = ...\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "wordcloud = WordCloud(...                    # specify white background\n",
    "                     random_state=42         # set seeed\n",
    "                     ).generate(...)         # generate from text dataset\n",
    "\n",
    "\n",
    "# plot \n",
    "plt.imshow(wordcloud,                  # specify wordcloud\n",
    "           interpolation = 'bilinear') # specifies how the words are displayed\n",
    "plt.axis('off')                        # turn off axes\n",
    "plt.show()                             # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: This is an example where using [regular expressions](https://docs.python.org/3/library/re.html) can be useful. Instead of inputting all of the different ways that something like \"XX\" might show up, you can use regex to find and remove all similar patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lengths and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the basic things we might look for when analyzing text data is the length of a document. Let's see how we might grab the total number of characters and the total number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature columns with counts of # of characters and # of words\n",
    "# ----------\n",
    "\n",
    "# count number of characters\n",
    "cfpb['complaint_len'] = cfpb['Consumer complaint narrative'].apply(len)\n",
    "\n",
    "# count number of words\n",
    "cfpb['word_count'] = cfpb['Consumer complaint narrative'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of complaint length (number of characters)\n",
    "sns.displot(...,               # specify data\n",
    "            x=\"complaint_len\") # x-axis feature\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of word count\n",
    "sns.displot(...,            # specify data\n",
    "            x=\"word_count\") # x-axis feature\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of word count by loan product \n",
    "sns.displot(...,              # specify data\n",
    "            x=\"word_count\",   # x-axis feature\n",
    "            hue = \"Product\",  # color by loan product\n",
    "            col = \"Product\")  # color by loan product\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common area of research in the social sciences is thinking about the \"sentiment\" of a text. The [`TextBlob`](https://textblob.readthedocs.io/en/dev/quickstart.html) library gives us access to a pre-trained sentiment analysis model. Text might be characterized as \"positive,\" \"negative,\" or \"neutral\" on a [-1,1] scale with -1 being highly negative and 1 being highly positive. Before we look at the code, do you expect that the sentiment scores for these data should be negative or positive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature colum of sentiment polarity\n",
    "# ---------- \n",
    "# create the \"tokens\" column again \n",
    "cfpb['tokens'] = cfpb['tokens'].map(lambda text: ' '.join(text))\n",
    "\n",
    "# create the \"tokens\" column again \n",
    "cfpb['polarity'] = cfpb['tokens'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "# view\n",
    "cfpb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of polarity\n",
    "sns.displot(...,         # specify data\n",
    "            x=\"...\")     # x-axis label \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Why does sentiment look so close to neural, or even slightly positive? We know that all of the narratives in this dataset are consumer **complaints**, so we should expect them to look somewhat negative. Let's look at the 5 most positive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to view the 5 most positive reviews\n",
    "# ---------- \n",
    "for complaint in cfpb.nlargest(5, 'polarity')['Consumer complaint narrative']:\n",
    "    print(complaint + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have any words that are skewing things? Let's look at the sentiment score for this first comment, and the individual sentiments of the words in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on the first complaint and view its overall polarity \n",
    "# ---------- \n",
    "# create string capturing the first complaint above for analysis\n",
    "sample_complaint = \"the company said they are offering a covid relief program which I requested assistance and they are saying a balloon payment is owed in XXXX I called the company and I was told that if I can't make this payment they will be talking taking litigation steps how are people who have lost their job able to keep their homes\"\n",
    "\n",
    "# print polarity score\n",
    "print(\"overall polarity score is \", TextBlob(sample_complaint).sentiment.polarity)\n",
    "for word in sample_complaint.split():\n",
    "    print(word, TextBlob(word).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one word actually has a sentiment score (\"able\")! TextBlob's sentiment polarity is not a simple average of all of the sentiments in a string - this is why preprocessing is important and why you should validate these types of off-the-shelf methods. Let's take a look at the most negative reviews and see if these make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to view the 5 most negative reviews\n",
    "# ---------- \n",
    "for complaint in cfpb.nsmallest(5, 'polarity')['Consumer complaint narrative']:\n",
    "    print(complaint + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on the first complaint and view its overall polarity \n",
    "# ---------- \n",
    "# create string capturing the first complaint above for analysis\n",
    "sample_complaint = \"Navient is the worst company to ever exist. Website does not work. Do the people at customer service even work for navient??? They don't know anything about whats going on. Applied for a repayement plan and their website always says an error has occured.\"\n",
    "\n",
    "# print polarity score\n",
    "print(\"overall polarity score is \", TextBlob(sample_complaint).sentiment.polarity)\n",
    "for word in sample_complaint.split():\n",
    "    print(word, TextBlob(word).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have a perfectly negative sentiment (-1.0), but again only one word is contirbuting - \"worst\". Sentiment polarity is a powerful tool, but not automatically suited to inference. That being said, maybe it can be helpful for distinguishing between labels. We can take a look at how polarity differs across mortgage and student loans. \n",
    "\n",
    "Is cutting the data this way helpful or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of polarity by loan product\n",
    "sns.displot(...,             # specify data\n",
    "            x=\"polarity\",    # specify x-axix feature \n",
    "            hue = \"Product\", # color by loan product\n",
    "            col = \"Product\") # color by loan product\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "In addition to sentiment polarity, TextBlob also has a method for determining how \"objective\" or \"subjective\" a piece of text is. Plot the objectivity measure by loan product. Do these results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create measure of subjectivity\n",
    "# ---------- \n",
    "\n",
    "# create new column feature of subjectivity\n",
    "cfpb['subjectivity'] = cfpb[...].map(lambda... .sentiment.subjectivity)\n",
    "\n",
    "# plot\n",
    "sns.displot(...) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScatterText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll take a look at a useful visualization for finding separarting words. We'll use the [ScatterText](https://spacy.io/universe/project/scattertext) library to visualize both word frequencies and how well they separate two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create measure of subjectivity\n",
    "# ---------- \n",
    "corpus = st.CorpusFromPandas(cfpb[:5000],              # specify data\n",
    "                             category_col = 'Product', # specify the explanatory variable  \n",
    "                             text_col = 'tokens',      # specify the text column\n",
    "                             nlp = nlp).build()        # apply the nlp algorithim and build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create html document\n",
    "html = st.produce_scattertext_explorer(corpus,\n",
    "                                       category='Student loan',\n",
    "                                       category_name='Student loan',\n",
    "                                       not_category_name='Mortgage',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       minimum_term_frequency=5,\n",
    "                                       metadata=cfpb['Complaint ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write html document to memory and open in browser\n",
    "open(\"CFPB Sentiment.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Methods\n",
    "\n",
    "Finally, we'll look at unsupervised machine learning methods for text data. Specifically, we'll implement [Kmeans Clustering], which is a simple and commonly used unsupervised machine learning methods, and [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), which is a classic method for topic modeling. Topic models can help us uncover structure within a text. Specifically it does so through a \"mixture model\" - meaning every document is assumed to be \"about\" various topics, and we try to estimate the proportion each topic contributes to a document. Let's reload our cfpb dataset and look at \"checkings or savings account\" and \"student loan\" products this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "# ---------- \n",
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative'])\n",
    "cfpb = cfpb[(cfpb['Product']=='Checking or savings account') | (cfpb['Product'] == 'Student loan')]\n",
    "cfpb = cfpb[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating our tf-idf matrix again. Note that this might take a bit because were are working with 1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf matrix\n",
    "# ----------\n",
    "# set X dataset\n",
    "X = cfpb['Consumer complaint narrative']        \n",
    "\n",
    "# initialize tf-idf using our preprocessing function\n",
    "tf = TfidfVectorizer(tokenizer = ...,           # use our function for tokenizing created above\n",
    "                     token_pattern = None)      # set to \"None\" since we have specify our own pattern\n",
    "\n",
    "# fit to data\n",
    "tfidf_matrix =  tf....\n",
    "\n",
    "# create dense matrix to view\n",
    "dense_matrix = tfidf_matrix....\n",
    "dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a useful visualization of tf-idf matrix\n",
    "# ----------\n",
    "\n",
    "# convert matrix to an arrray and then to a dataframe\n",
    "tf_idf_df = pd.DataFrame(data = tfidf_matrix.toarray(),      # convert to array than to datafram\n",
    "                         columns=tf.get_feature_names_out()) # specify column names as feature names \n",
    "\n",
    "# sort by term frequency on the first document\n",
    "tf_idf_df.T.nlargest(20,  # transpose the matrix = columns are not what were rows (documents)\n",
    "                      0)  # on column index 0 to show the largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering\n",
    "\n",
    "Let's implement a simple kmeans clustering algorithim and add those labels (which cluster each record belongs to) back to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans clustering\n",
    "# ---------- \n",
    "# import kmean library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# implement kmeans clustering\n",
    "kmeans = KMeans(...,                # specify 3 of clusters\n",
    "                ...                 # specify 300 of iterations\n",
    "                ).fit(tfidf_matrix) # specify data to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the centers of the clusters through the cluster_centers_ attribute. To get the labels \n",
    "#(i.e. the corresponding cluster) we use labels_.\n",
    "cfpb['cluster']= kmeans.labels_    # add labels to original data frame\n",
    "cfpb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Now that we have a dense matrix, let's apply our LDA model. The key hyperparameter here is the `n_components` argument. Let's start with 5, and then print out our topics to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA \n",
    "# ---------- \n",
    "# initialize LDA model \n",
    "lda = LatentDirichletAllocation(...,              # set # of components to 5\n",
    "                                ...,              # specify 20 max iterations\n",
    "                                random_state=0)   # set seed for reproducibility\n",
    "\n",
    "# fit LDA to data\n",
    "lda = lda.fit(tfidf_matrix)  # LDA no longer accepts dense matrix; could use this alternate code: dense_array = np.asarray(dense_matrix)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get and view top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):    # define function and parameters\n",
    "    for topic_idx, topic in enumerate(model.components_):  # iterate over each topic \n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))            # print topic index\n",
    "        print(\" \".join([feature_names[i]                   # print topics\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names from tf-idf matrices \n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# apply function to print top words\n",
    "print_top_words(...,                # model \n",
    "                tf_feature_names,   # feature names \n",
    "                ...)                # top 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 topics! Some seem to be sensible (i.e. Topic 1 seems to be about banking while Topic 2 is about student loans), but notice that the computer doedsn't find the right \"topic names\" for us automatically - so there is still a role for humans to interpret and make sense of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic weights\n",
    "\n",
    "One thing we may want to do with the output is compare the prevalence of each topic across documents. A simple way to do this, is to merge the topic distribution back into the `pandas` dataframe.\n",
    "\n",
    "Let's first get the topic distribution array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic weights \n",
    "# ----------\n",
    "topic_dist = lda.transform(tfidf_matrix)\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we'll merge back with original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert topic_dist to pandas dataframe\n",
    "topic_dist_df = pd.DataFrame(topic_dist)\n",
    "\n",
    "# join back to pandas dataframe and reset index\n",
    "df_w_topics = topic_dist_df.join(cfpb.reset_index())\n",
    "\n",
    "# view\n",
    "df_w_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the average weight of each topic across loan product using `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by loan products and calculate mean\n",
    "grouped = df_w_topics.groupby('Product')\n",
    "for i in range(0, 5):\n",
    "    print(grouped[i].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the topics seem to have much separation, but let's visualize topics 2 and 3 to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of topic 2\n",
    "sns.displot(df_w_topics,      # data\n",
    "            x=df_w_topics[2], # x-axis feature subet to only topic 2\n",
    "            hue = \"Product\",  # by loan product\n",
    "            kind = 'kde',     # specify kdensity plot\n",
    "            fill = 'true')    # fill\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of topic 3\n",
    "sns.displot(df_w_topics,      # data\n",
    "            x=df_w_topics[3], # x-axis feature subet to only topic 2\n",
    "            hue = \"Product\",  # by loan product\n",
    "            kind = 'kde',     # specify kdensity plot\n",
    "            fill = 'true')    # fill\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, there doesn't seem to be very much separation between the two topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Try retraining the LDA witha  different number of topics, say 10. What do you notice? How is this similar to issues we've seen with other clustering algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA - 10 topics instead\n",
    "# ---------- \n",
    "\n",
    "# initialize LDA model \n",
    "lda = LatentDirichletAllocation(...,             # specify 10 components\n",
    "                                ...,             # set max iter to 20\n",
    "                                random_state=0)  # set random seed\n",
    "\n",
    "\n",
    "# fit LDA to data\n",
    "lda = lda.fit(...)\n",
    "\n",
    "# define a function to get and view top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):   # define function and parameters\n",
    "    for topic_idx, topic in enumerate(model.components_): # iterate over each topic \n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))           # print topic index\n",
    "        print(\" \".join([feature_names[i]                  # print topics\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# get feature names from tf-idf matrices \n",
    "tf_feature_names = ...\n",
    "\n",
    "# apply function to print top 20 words\n",
    "print_top_words(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by Aniket Kesari. Modified by Prashant Sharma (2023) and annotated by Kasey Zapatka (2024)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
